{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5570fe7a",
   "metadata": {},
   "source": [
    "# Pre-processing & Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94e3cae",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c2aa54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8692c7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>post_text</th>\n",
       "      <th>id</th>\n",
       "      <th>score</th>\n",
       "      <th>total_comments</th>\n",
       "      <th>post_url</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_type</th>\n",
       "      <th>title_&amp;_text</th>\n",
       "      <th>title_text_stemmed</th>\n",
       "      <th>title_text_lemmatized</th>\n",
       "      <th>trending</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Daily Fasting Check-in!</td>\n",
       "      <td>* **Type** of fast (water, juice, smoking, etc...</td>\n",
       "      <td>16o7z6r</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>https://www.reddit.com/r/intermittentfasting/c...</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>hot</td>\n",
       "      <td>Daily Fasting Check-in! * **Type** of fast (wa...</td>\n",
       "      <td>['daili', 'checkin', 'type', 'fast', 'water', ...</td>\n",
       "      <td>['daily', 'checkin', 'type', 'fast', 'water', ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I decided who I wanted to be and I became her üíÖüèΩ</td>\n",
       "      <td>So a little background: I‚Äôm 39, have birthed t...</td>\n",
       "      <td>16ntqoy</td>\n",
       "      <td>1176</td>\n",
       "      <td>36</td>\n",
       "      <td>https://i.redd.it/fclkjnwhmgpb1.jpg</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>hot</td>\n",
       "      <td>I decided who I wanted to be and I became her ...</td>\n",
       "      <td>['decid', 'want', 'becam', 'littl', 'backgroun...</td>\n",
       "      <td>['decided', 'wanted', 'became', 'little', 'bac...</td>\n",
       "      <td>42336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Some photos from a past vacation came up as a ...</td>\n",
       "      <td>I remember being miserable and insecure the en...</td>\n",
       "      <td>16ni914</td>\n",
       "      <td>1505</td>\n",
       "      <td>77</td>\n",
       "      <td>https://www.reddit.com/gallery/16ni914</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>hot</td>\n",
       "      <td>Some photos from a past vacation came up as a ...</td>\n",
       "      <td>['photo', 'past', 'vacat', 'came', 'memori', '...</td>\n",
       "      <td>['photo', 'past', 'vacation', 'came', 'memory'...</td>\n",
       "      <td>115885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anybody find IF, lose weight, and then lose mo...</td>\n",
       "      <td>I know I am an idiot.</td>\n",
       "      <td>16nuqx9</td>\n",
       "      <td>198</td>\n",
       "      <td>78</td>\n",
       "      <td>https://www.reddit.com/r/intermittentfasting/c...</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>hot</td>\n",
       "      <td>Anybody find IF, lose weight, and then lose mo...</td>\n",
       "      <td>['anybodi', 'find', 'lose', 'weight', 'lose', ...</td>\n",
       "      <td>['anybody', 'find', 'lose', 'weight', 'lose', ...</td>\n",
       "      <td>15444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2 and a half months of IF</td>\n",
       "      <td>From 234 to 211 in 2.5 months. It works! Once ...</td>\n",
       "      <td>16nuxqs</td>\n",
       "      <td>180</td>\n",
       "      <td>12</td>\n",
       "      <td>https://i.redd.it/30yqmtsdvgpb1.jpg</td>\n",
       "      <td>intermittentfasting</td>\n",
       "      <td>hot</td>\n",
       "      <td>2 and a half months of IF From 234 to 211 in 2...</td>\n",
       "      <td>['2', 'half', 'month', '234', '211', '25', 'mo...</td>\n",
       "      <td>['2', 'half', 'month', '234', '211', '25', 'mo...</td>\n",
       "      <td>2160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                            Daily Fasting Check-in!   \n",
       "1   I decided who I wanted to be and I became her üíÖüèΩ   \n",
       "2  Some photos from a past vacation came up as a ...   \n",
       "3  Anybody find IF, lose weight, and then lose mo...   \n",
       "4                          2 and a half months of IF   \n",
       "\n",
       "                                           post_text       id  score  \\\n",
       "0  * **Type** of fast (water, juice, smoking, etc...  16o7z6r      1   \n",
       "1  So a little background: I‚Äôm 39, have birthed t...  16ntqoy   1176   \n",
       "2  I remember being miserable and insecure the en...  16ni914   1505   \n",
       "3                              I know I am an idiot.  16nuqx9    198   \n",
       "4  From 234 to 211 in 2.5 months. It works! Once ...  16nuxqs    180   \n",
       "\n",
       "   total_comments                                           post_url  \\\n",
       "0               2  https://www.reddit.com/r/intermittentfasting/c...   \n",
       "1              36                https://i.redd.it/fclkjnwhmgpb1.jpg   \n",
       "2              77             https://www.reddit.com/gallery/16ni914   \n",
       "3              78  https://www.reddit.com/r/intermittentfasting/c...   \n",
       "4              12                https://i.redd.it/30yqmtsdvgpb1.jpg   \n",
       "\n",
       "             subreddit post_type  \\\n",
       "0  intermittentfasting       hot   \n",
       "1  intermittentfasting       hot   \n",
       "2  intermittentfasting       hot   \n",
       "3  intermittentfasting       hot   \n",
       "4  intermittentfasting       hot   \n",
       "\n",
       "                                        title_&_text  \\\n",
       "0  Daily Fasting Check-in! * **Type** of fast (wa...   \n",
       "1  I decided who I wanted to be and I became her ...   \n",
       "2  Some photos from a past vacation came up as a ...   \n",
       "3  Anybody find IF, lose weight, and then lose mo...   \n",
       "4  2 and a half months of IF From 234 to 211 in 2...   \n",
       "\n",
       "                                  title_text_stemmed  \\\n",
       "0  ['daili', 'checkin', 'type', 'fast', 'water', ...   \n",
       "1  ['decid', 'want', 'becam', 'littl', 'backgroun...   \n",
       "2  ['photo', 'past', 'vacat', 'came', 'memori', '...   \n",
       "3  ['anybodi', 'find', 'lose', 'weight', 'lose', ...   \n",
       "4  ['2', 'half', 'month', '234', '211', '25', 'mo...   \n",
       "\n",
       "                               title_text_lemmatized  trending  \n",
       "0  ['daily', 'checkin', 'type', 'fast', 'water', ...         2  \n",
       "1  ['decided', 'wanted', 'became', 'little', 'bac...     42336  \n",
       "2  ['photo', 'past', 'vacation', 'came', 'memory'...    115885  \n",
       "3  ['anybody', 'find', 'lose', 'weight', 'lose', ...     15444  \n",
       "4  ['2', 'half', 'month', '234', '211', '25', 'mo...      2160  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Sheila's df as csv\n",
    "reddits = pd.read_csv('reddits.csv')\n",
    "\n",
    "reddits.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd6f329",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fdc9a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       1\n",
       "2       1\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "2484    0\n",
       "2485    0\n",
       "2486    0\n",
       "2487    0\n",
       "2488    0\n",
       "Name: subreddit_binarized, Length: 2489, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Binarize 'subreddit' for modelling\n",
    "## 'AnorexiaNervosa' = 0\n",
    "## 'intermittentfasting' = 1\n",
    "\n",
    "reddits['subreddit_binarized'] = reddits['subreddit'].map({'AnorexiaNervosa': 0, 'intermittentfasting': 1})\n",
    "\n",
    "reddits['subreddit_binarized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d8f1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble features (x) and target (y)\n",
    "\n",
    "X = reddits['title_text_stemmed'].tolist()\n",
    "y = reddits['subreddit_binarized'].tolist()\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2664ea5d",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfe1dab",
   "metadata": {},
   "source": [
    "From the EDA, **we have decided to use the stemmed texts** instead of lemmatized texts as they provide more insights rather than variations of the same words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477aaacb",
   "metadata": {},
   "source": [
    "### Functions for the 3 vectorization methods\n",
    "* Count vectorizer\n",
    "* N-grams\n",
    "* TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98fca0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for CountVectorizer for unigrams\n",
    "\n",
    "def cv_unigram(data):\n",
    "    \n",
    "    # Instantiate CountVectorizer \n",
    "    cv = CountVectorizer()\n",
    "    \n",
    "    # X stores the vectorized version of the data\n",
    "    X = cv.fit_transform(data)\n",
    "    \n",
    "    return X, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e24ede62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for CountVectorizer for bigrams\n",
    "\n",
    "def cv_bigram(data):\n",
    "    \n",
    "    # Instantiate CountVectorizer \n",
    "    cv = CountVectorizer(ngram_range=(2,2))\n",
    "    \n",
    "    # X stores the vectorized version of the data\n",
    "    X = cv.fit_transform(data)\n",
    "    \n",
    "    return X, cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3e3ccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for TF-IDF\n",
    "\n",
    "def tfidf(data):\n",
    "    \n",
    "    #Instantiate TfidfVectorizer\n",
    "    tfidf = TfidfVectorizer()\n",
    "    X = tfidf.fit_transform(data)\n",
    "    \n",
    "    return X, tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7849b886",
   "metadata": {},
   "source": [
    "### Functions for the 3 Naive Bayes (NB) models\n",
    "* Bernoulli NB\n",
    "* Multinomial NB\n",
    "* Gaussian NB\n",
    "\n",
    "### Function for SMOTE to account for the class imbalance\n",
    "* r/AnorexiaNervosa: 1588 posts\n",
    "* r/intermittentfasting: 901 posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d50affad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for Bernoulli NB\n",
    "\n",
    "def bernoulli(X_train, X_test, y_train, y_test):\n",
    "\n",
    "    # Apply SMOTE to the training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Instantiate the Bernoulli model\n",
    "    BernNB = BernoulliNB(binarize=0.1)\n",
    "    \n",
    "    # Fit the model\n",
    "    BernNB.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train_resampled = BernNB.predict(X_train_resampled)\n",
    "    y_pred_test = BernNB.predict(X_test)\n",
    "    \n",
    "    # Get score\n",
    "    train_score = accuracy_score(y_train_resampled, y_pred_train_resampled)\n",
    "    test_score = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    return train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "670f4538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for Multinomial NB\n",
    "\n",
    "def multinomial(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # Apply SMOTE to the training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Instantiate the Multinomial model\n",
    "    MultiNB = MultinomialNB()\n",
    "    \n",
    "    # Fit the model\n",
    "    MultiNB.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train_resampled = MultiNB.predict(X_train_resampled)\n",
    "    y_pred_test = MultiNB.predict(X_test)\n",
    "    \n",
    "    # Get score\n",
    "    train_score = accuracy_score(y_train_resampled, y_pred_train_resampled)\n",
    "    test_score = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    return train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca722955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for Gaussian NB\n",
    "\n",
    "def gaussian(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # Apply SMOTE to the training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Instantiate the Gaussian model\n",
    "    GausNB = GaussianNB()\n",
    "    \n",
    "    # Fit the model\n",
    "    GausNB.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train_resampled = GausNB.predict(X_train_resampled)\n",
    "    y_pred_test = GausNB.predict(X_test)\n",
    "    \n",
    "    # Get score\n",
    "    train_score = accuracy_score(y_train_resampled, y_pred_train_resampled)\n",
    "    test_score = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    return train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6feae811",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for cross-validation\n",
    "\n",
    "def cross_val(X, y, model):\n",
    "    \n",
    "    cross_val = cross_val_score(model, X, y, cv=5)\n",
    "    \n",
    "    return cross_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b98eea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data using different methods\n",
    "X_cv_uni, cv_obj_uni = cv_unigram(X_train)\n",
    "X_cv_bi, cv_obj_bi = cv_bigram(X_train)\n",
    "X_tf, tf_obj = tfidf(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6013b888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli NB:\n",
      "Unigram:\n",
      "Train: 0.8902053712480252, Test: 0.8333333333333334\n",
      "\n",
      "Bigram:\n",
      "Train: 0.8143759873617693, Test: 0.44779116465863456\n",
      "\n",
      "TF-IDF:\n",
      "Train: 0.9778830963665087, Test: 0.9397590361445783\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the Bernoulli NB model\n",
    "train_bern_uni_score, test_bern_uni_score = bernoulli(X_cv_uni, cv_obj_uni.transform(X_test), y_train, y_test)\n",
    "train_bern_bi_score, test_bern_bi_score = bernoulli(X_cv_bi, cv_obj_bi.transform(X_test), y_train, y_test)\n",
    "train_bern_tf_score, test_bern_tf_score = bernoulli(X_tf, tf_obj.transform(X_test), y_train, y_test)\n",
    "\n",
    "print('Bernoulli NB:')\n",
    "print('Unigram:')\n",
    "print(f'Train: {train_bern_uni_score}, Test: {test_bern_uni_score}')\n",
    "print()\n",
    "print('Bigram:')\n",
    "print(f'Train: {train_bern_bi_score}, Test: {test_bern_bi_score}')\n",
    "print()\n",
    "print('TF-IDF:')\n",
    "print(f'Train: {train_bern_tf_score}, Test: {test_bern_tf_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74b21c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation:\n",
      "Unigram scores: [0.95238095 0.94974874 0.94221106 0.96733668 0.95477387]\n",
      "Unigram mean: 0.9532902608279492\n",
      "\n",
      "Bigram scores: [0.64661654 0.67085427 0.65075377 0.66834171 0.67085427]\n",
      "Bigram mean: 0.6614841122907772\n",
      "\n",
      "TF-IDF scores: [0.95238095 0.94974874 0.94221106 0.96733668 0.95477387]\n",
      "TF-IDF mean: 0.9532902608279492\n"
     ]
    }
   ],
   "source": [
    "#Instantiate the Bernoulli model\n",
    "BernNB = BernoulliNB()\n",
    "\n",
    "# Cross-validation on the Bernoulli NB model using unigram features\n",
    "cross_val_uni_scores = cross_val(X_cv_uni, y_train, BernNB)\n",
    "cross_val_uni_bern = (cross_val(X_cv_uni, y_train, BernNB)).mean()\n",
    "\n",
    "# Cross-validation on the Bernoulli NB model using bigram features\n",
    "cross_val_bi_scores = cross_val(X_cv_bi, y_train, BernNB)\n",
    "cross_val_bi_bern = (cross_val(X_cv_bi, y_train, BernNB)).mean()\n",
    "\n",
    "# Cross-validation on the Bernoulli NB model using TF-IDF features\n",
    "cross_val_tf_scores = cross_val(X_tf, y_train, BernNB)\n",
    "cross_val_tf_bern = (cross_val(X_tf, y_train, BernNB)).mean()\n",
    "\n",
    "print('Cross-validation:')\n",
    "print(f'Unigram scores: {cross_val_uni_scores}')\n",
    "print(f'Unigram mean: {cross_val_uni_bern}')\n",
    "print()\n",
    "print(f'Bigram scores: {cross_val_bi_scores}')\n",
    "print(f'Bigram mean: {cross_val_bi_bern}')\n",
    "print()\n",
    "print(f'TF-IDF scores: {cross_val_tf_scores}')\n",
    "print(f'TF-IDF mean: {cross_val_tf_bern}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87adcc8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multinomial NB:\n",
      "Unigram:\n",
      "Train: 0.9451026856240127, Test: 0.9518072289156626\n",
      "\n",
      "Bigrams:\n",
      "Train: 0.8554502369668247, Test: 0.9397590361445783\n",
      "\n",
      "TF-IDF:\n",
      "Train: 0.9826224328593997, Test: 0.9497991967871486\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the Multinomial NB model\n",
    "train_multi_uni_score, test_multi_uni_score = multinomial(X_cv_uni, cv_obj_uni.transform(X_test), y_train, y_test)\n",
    "train_multi_bi_score, test_multi_bi_score = multinomial(X_cv_bi, cv_obj_bi.transform(X_test), y_train, y_test)\n",
    "train_multi_tf_score, test_multi_tf_score = multinomial(X_tf, tf_obj.transform(X_test), y_train, y_test)\n",
    "\n",
    "print('Multinomial NB:')\n",
    "print('Unigram:')\n",
    "print(f'Train: {train_multi_uni_score}, Test: {test_multi_uni_score}')\n",
    "print()\n",
    "print('Bigrams:')\n",
    "print(f'Train: {train_multi_bi_score}, Test: {test_multi_bi_score}')\n",
    "print()\n",
    "print('TF-IDF:')\n",
    "print(f'Train: {train_multi_tf_score}, Test: {test_multi_tf_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4738cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation:\n",
      "Unigram scores: [0.95739348 0.96231156 0.93467337 0.95477387 0.95477387]\n",
      "Unigram mean: 0.9527852294051712\n",
      "\n",
      "Bigram scores: [0.73684211 0.74623116 0.74371859 0.75879397 0.72613065]\n",
      "Bigram mean: 0.7423432954244908\n",
      "\n",
      "TF-IDF scores: [0.87719298 0.85678392 0.84924623 0.87437186 0.85929648]\n",
      "TF-IDF mean: 0.8633782949836905\n"
     ]
    }
   ],
   "source": [
    "#Instantiate the Multinomial model\n",
    "MultiNB = MultinomialNB()\n",
    "\n",
    "# Cross-validation on the Multinomial NB model using unigram features\n",
    "cross_val_uni_scores = cross_val(X_cv_uni, y_train, MultiNB)\n",
    "cross_val_uni_multi = (cross_val(X_cv_uni, y_train, MultiNB)).mean()\n",
    "\n",
    "# Cross-validation on the Multinomial NB model using bigram features\n",
    "cross_val_bi_scores = cross_val(X_cv_bi, y_train, MultiNB)\n",
    "cross_val_bi_multi = (cross_val(X_cv_bi, y_train, MultiNB)).mean()\n",
    "\n",
    "# Cross-validation on the Multinomial NB model using TF-IDF features\n",
    "cross_val_tf_scores = cross_val(X_tf, y_train, MultiNB)\n",
    "cross_val_tf_multi = (cross_val(X_tf, y_train, MultiNB)).mean()\n",
    "\n",
    "print('Cross-validation:')\n",
    "print(f'Unigram scores: {cross_val_uni_scores}')\n",
    "print(f'Unigram mean: {cross_val_uni_multi}')\n",
    "print()\n",
    "print(f'Bigram scores: {cross_val_bi_scores}')\n",
    "print(f'Bigram mean: {cross_val_bi_multi}')\n",
    "print()\n",
    "print(f'TF-IDF scores: {cross_val_tf_scores}')\n",
    "print(f'TF-IDF mean: {cross_val_tf_multi}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cecf783e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian NB:\n",
      "Unigram:\n",
      "Train: 0.9514218009478673, Test: 0.8232931726907631\n",
      "\n",
      "Bigram:\n",
      "Train: 1.0, Test: 0.8815261044176707\n",
      "\n",
      "TF-IDF:\n",
      "Train: 0.9652448657187994, Test: 0.8313253012048193\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the Gaussian NB model\n",
    "train_gaus_uni_score, test_gaus_uni_score = gaussian(X_cv_uni.toarray(), cv_obj_uni.transform(X_test).toarray(), y_train, y_test)\n",
    "train_gaus_bi_score, test_gaus_bi_score = gaussian(X_cv_bi.toarray(), cv_obj_bi.transform(X_test).toarray(), y_train, y_test)\n",
    "train_gaus_tf_score, test_gaus_tf_score = gaussian(X_tf.toarray(), tf_obj.transform(X_test).toarray(), y_train, y_test)\n",
    "\n",
    "print('Gaussian NB:')\n",
    "print('Unigram:')\n",
    "print(f'Train: {train_gaus_uni_score}, Test: {test_gaus_uni_score}')\n",
    "print()\n",
    "print('Bigram:')\n",
    "print(f'Train: {train_gaus_bi_score}, Test: {test_gaus_bi_score}')\n",
    "print()\n",
    "print('TF-IDF:')\n",
    "print(f'Train: {train_gaus_tf_score}, Test: {test_gaus_tf_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa33d9cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation:\n",
      "Unigram scores: [0.82957393 0.83919598 0.79648241 0.80150754 0.77889447]\n",
      "Unigram mean: 0.8091308673694286\n",
      "\n",
      "Bigram scores: [0.88220551 0.84924623 0.86180905 0.8919598  0.85678392]\n",
      "Bigram mean: 0.868400901751867\n",
      "\n",
      "TF-IDF scores: [0.81954887 0.83417085 0.80653266 0.78643216 0.77135678]\n",
      "TF-IDF mean: 0.8036082668984017\n"
     ]
    }
   ],
   "source": [
    "#Instantiate the Gaussian model\n",
    "GausNB = GaussianNB()\n",
    "\n",
    "# Cross-validation on the Gaussian NB model using unigram features\n",
    "cross_val_uni_scores = cross_val(X_cv_uni.toarray(), y_train, GausNB)\n",
    "cross_val_uni_gaus = (cross_val(X_cv_uni.toarray(), y_train, GausNB)).mean()\n",
    "\n",
    "# Cross-validation on the Gaussian NB model using bigram features\n",
    "cross_val_bi_scores = cross_val(X_cv_bi.toarray(), y_train, GausNB)\n",
    "cross_val_bi_gaus = (cross_val(X_cv_bi.toarray(), y_train, GausNB)).mean()\n",
    "\n",
    "# Cross-validation on the Gaussian NB model using TF-IDF features\n",
    "cross_val_tf_scores = cross_val(X_tf.toarray(), y_train, GausNB)\n",
    "cross_val_tf_gaus = (cross_val(X_tf.toarray(), y_train, GausNB)).mean()\n",
    "\n",
    "print('Cross-validation:')\n",
    "print(f'Unigram scores: {cross_val_uni_scores}')\n",
    "print(f'Unigram mean: {cross_val_uni_gaus}')\n",
    "print()\n",
    "print(f'Bigram scores: {cross_val_bi_scores}')\n",
    "print(f'Bigram mean: {cross_val_bi_gaus}')\n",
    "print()\n",
    "print(f'TF-IDF scores: {cross_val_tf_scores}')\n",
    "print(f'TF-IDF mean: {cross_val_tf_gaus}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8befbcc0",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14467088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function for XGBoost classifier\n",
    "\n",
    "def xgboost(X_train, X_test, y_train, y_test):\n",
    "    \n",
    "    # Apply SMOTE to the training data\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Instantiate the XGBoost classifier\n",
    "    xgb_classifier = XGBClassifier()\n",
    "    \n",
    "    # Fit the XGBoost classifier on the resampled training data\n",
    "    xgb_classifier.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Predict the labels for both training and test data\n",
    "    y_pred_train = xgb_classifier.predict(X_train)\n",
    "    y_pred_test = xgb_classifier.predict(X_test)\n",
    "    \n",
    "    # Get score\n",
    "    train_score = accuracy_score(y_train, y_pred_train)\n",
    "    test_score = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    return train_score, test_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "935c734e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier:\n",
      "Unigram:\n",
      "Train: 0.9964841788046208, Test: 0.9377510040160643\n",
      "\n",
      "Bigram:\n",
      "Train: 0.8940231039678553, Test: 0.8493975903614458\n",
      "\n",
      "TF-IDF:\n",
      "Train: 0.9969864389753893, Test: 0.9337349397590361\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate the XGBoost classifier model\n",
    "train_xgb_uni_score, test_xgb_uni_score = xgboost(X_cv_uni, cv_obj_uni.transform(X_test), y_train, y_test)\n",
    "train_xgb_bi_score, test_xgb_bi_score = xgboost(X_cv_bi, cv_obj_bi.transform(X_test), y_train, y_test)\n",
    "train_xgb_tf_score, test_xgb_tf_score = xgboost(X_tf, tf_obj.transform(X_test), y_train, y_test)\n",
    "\n",
    "print('XGBoost Classifier:')\n",
    "print('Unigram:')\n",
    "print(f'Train: {train_xgb_uni_score}, Test: {test_xgb_uni_score}')\n",
    "print()\n",
    "print('Bigram:')\n",
    "print(f'Train: {train_xgb_bi_score}, Test: {test_xgb_bi_score}')\n",
    "print()\n",
    "print('TF-IDF:')\n",
    "print(f'Train: {train_xgb_tf_score}, Test: {test_xgb_tf_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a9b3e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation:\n",
      "Unigram scores: [0.94235589 0.93467337 0.95226131 0.92713568 0.95477387]\n",
      "Unigram mean: 0.9527852294051712\n",
      "\n",
      "Bigram scores: [0.80701754 0.83165829 0.78140704 0.80653266 0.8040201 ]\n",
      "Bigram mean: 0.7423432954244908\n",
      "\n",
      "TF-IDF scores: [0.94736842 0.92713568 0.94974874 0.9120603  0.94472362]\n",
      "TF-IDF mean: 0.8633782949836905\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the XGBoost Classifier model\n",
    "xgb_classifier = XGBClassifier()\n",
    "\n",
    "# Cross-validation on the XGBoost Classifier model using unigram features\n",
    "cross_val_uni_scores = cross_val(X_cv_uni, y_train, xgb_classifier)\n",
    "cross_val_uni_xgb = (cross_val(X_cv_uni, y_train, xgb_classifier)).mean()\n",
    "\n",
    "# Cross-validation on the XGBoost Classifier model using bigram features\n",
    "cross_val_bi_scores = cross_val(X_cv_bi, y_train, xgb_classifier)\n",
    "cross_val_bi_xgb = (cross_val(X_cv_bi, y_train, xgb_classifier)).mean()\n",
    "\n",
    "# Cross-validation on the XGBoost Classifier model using TF-IDF features\n",
    "cross_val_tf_scores = cross_val(X_tf, y_train, xgb_classifier)\n",
    "cross_val_tf_xgb = (cross_val(X_tf, y_train, xgb_classifier)).mean()\n",
    "\n",
    "print('Cross-validation:')\n",
    "print(f'Unigram scores: {cross_val_uni_scores}')\n",
    "print(f'Unigram mean: {cross_val_uni_multi}')\n",
    "print()\n",
    "print(f'Bigram scores: {cross_val_bi_scores}')\n",
    "print(f'Bigram mean: {cross_val_bi_multi}')\n",
    "print()\n",
    "print(f'TF-IDF scores: {cross_val_tf_scores}')\n",
    "print(f'TF-IDF mean: {cross_val_tf_multi}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "92e187f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Vectorizer</th>\n",
       "      <th>Cross-validation</th>\n",
       "      <th>Train</th>\n",
       "      <th>Test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bernoulli NB</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.953290</td>\n",
       "      <td>0.890205</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Multinomial NB</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.661484</td>\n",
       "      <td>0.814376</td>\n",
       "      <td>0.447791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gaussian NB</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.953290</td>\n",
       "      <td>0.977883</td>\n",
       "      <td>0.939759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>CountVectorizer</td>\n",
       "      <td>0.952785</td>\n",
       "      <td>0.945103</td>\n",
       "      <td>0.951807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bernoulli NB</td>\n",
       "      <td>N-gram (2,2)</td>\n",
       "      <td>0.742343</td>\n",
       "      <td>0.855450</td>\n",
       "      <td>0.939759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Multinomial NB</td>\n",
       "      <td>N-gram (2,2)</td>\n",
       "      <td>0.863378</td>\n",
       "      <td>0.982622</td>\n",
       "      <td>0.949799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Gaussian NB</td>\n",
       "      <td>N-gram (2,2)</td>\n",
       "      <td>0.809131</td>\n",
       "      <td>0.951422</td>\n",
       "      <td>0.823293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>N-gram (2,2)</td>\n",
       "      <td>0.868401</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.881526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Bernoulli NB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.803608</td>\n",
       "      <td>0.965245</td>\n",
       "      <td>0.831325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Multinomial NB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.942240</td>\n",
       "      <td>0.996484</td>\n",
       "      <td>0.937751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Gaussian NB</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.806127</td>\n",
       "      <td>0.894023</td>\n",
       "      <td>0.849398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>TF-IDF</td>\n",
       "      <td>0.936207</td>\n",
       "      <td>0.996986</td>\n",
       "      <td>0.933735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model       Vectorizer  Cross-validation     Train      Test\n",
       "0     Bernoulli NB  CountVectorizer          0.953290  0.890205  0.833333\n",
       "1   Multinomial NB  CountVectorizer          0.661484  0.814376  0.447791\n",
       "2      Gaussian NB  CountVectorizer          0.953290  0.977883  0.939759\n",
       "3          XGBoost  CountVectorizer          0.952785  0.945103  0.951807\n",
       "4     Bernoulli NB     N-gram (2,2)          0.742343  0.855450  0.939759\n",
       "5   Multinomial NB     N-gram (2,2)          0.863378  0.982622  0.949799\n",
       "6      Gaussian NB     N-gram (2,2)          0.809131  0.951422  0.823293\n",
       "7          XGBoost     N-gram (2,2)          0.868401  1.000000  0.881526\n",
       "8     Bernoulli NB           TF-IDF          0.803608  0.965245  0.831325\n",
       "9   Multinomial NB           TF-IDF          0.942240  0.996484  0.937751\n",
       "10     Gaussian NB           TF-IDF          0.806127  0.894023  0.849398\n",
       "11         XGBoost           TF-IDF          0.936207  0.996986  0.933735"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {'Model': ['Bernoulli NB', 'Multinomial NB', 'Gaussian NB', 'XGBoost', \n",
    "                     'Bernoulli NB', 'Multinomial NB', 'Gaussian NB', 'XGBoost',\n",
    "                     'Bernoulli NB', 'Multinomial NB', 'Gaussian NB', 'XGBoost'],\n",
    "          'Vectorizer': ['CountVectorizer', 'CountVectorizer', 'CountVectorizer', 'CountVectorizer', \n",
    "                         'N-gram (2,2)', 'N-gram (2,2)', 'N-gram (2,2)', 'N-gram (2,2)', \n",
    "                         'TF-IDF', 'TF-IDF', 'TF-IDF', 'TF-IDF'],\n",
    "          'Cross-validation': [cross_val_uni_bern, cross_val_bi_bern, cross_val_tf_bern, \n",
    "                               cross_val_uni_multi, cross_val_bi_multi, cross_val_tf_multi, \n",
    "                               cross_val_uni_gaus, cross_val_bi_gaus, cross_val_tf_gaus,\n",
    "                               cross_val_uni_xgb, cross_val_bi_xgb, cross_val_tf_xgb],\n",
    "          'Train': [train_bern_uni_score, train_bern_bi_score, train_bern_tf_score,\n",
    "                    train_multi_uni_score, train_multi_bi_score, train_multi_tf_score,\n",
    "                    train_gaus_uni_score, train_gaus_bi_score, train_gaus_tf_score,\n",
    "                    train_xgb_uni_score, train_xgb_bi_score, train_xgb_tf_score],\n",
    "          'Test': [test_bern_uni_score, test_bern_bi_score, test_bern_tf_score,\n",
    "                   test_multi_uni_score, test_multi_bi_score, test_multi_tf_score,\n",
    "                   test_gaus_uni_score, test_gaus_bi_score, test_gaus_tf_score, \n",
    "                   test_xgb_uni_score, test_xgb_bi_score, test_xgb_tf_score]\n",
    "}\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a28e75",
   "metadata": {},
   "source": [
    "Train scores are significantly higher than the test scores and cross-validation scores. This indicates **overfitting across all 3 models**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
