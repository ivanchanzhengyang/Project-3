{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b616572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Third-party Library Imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import praw\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import concurrent.futures\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.util import bigrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f401b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas display options to show the entire content of the \"Post Text\" column\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50498c99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2489"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Specify the path to your CSV file\n",
    "csv_file_path = 'reddits.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df_reddit_post = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Now, 'df' contains your data from the CSV file\n",
    "\n",
    "len(df_reddit_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad49e1a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Post Text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Post Text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m post[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPost Text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m post[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPost URL\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.gif\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.gifv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.webp\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Apply the function to the DataFrame to create a Boolean mask\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m image_only_mask \u001b[38;5;241m=\u001b[39m df_reddit_post\u001b[38;5;241m.\u001b[39mapply(is_image_only_post, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Filter out image-only posts\u001b[39;00m\n\u001b[0;32m     10\u001b[0m filtered_df_reddit_post \u001b[38;5;241m=\u001b[39m df_reddit_post[\u001b[38;5;241m~\u001b[39mimage_only_mask]\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:9568\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[0;32m   9557\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m   9559\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m   9560\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   9561\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9566\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m   9567\u001b[0m )\n\u001b[1;32m-> 9568\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mapply()\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:764\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw()\n\u001b[1;32m--> 764\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:891\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 891\u001b[0m     results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_generator()\n\u001b[0;32m    893\u001b[0m     \u001b[38;5;66;03m# wrap results\u001b[39;00m\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:907\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    905\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m    906\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m--> 907\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf(v)\n\u001b[0;32m    908\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m    909\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m    910\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m    911\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m, in \u001b[0;36mis_image_only_post\u001b[1;34m(post)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_image_only_post\u001b[39m(post):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Check if the post's content (Post Text) is empty (no text) and the URL is an image or an image-hosting site\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m post[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPost Text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m post[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPost URL\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.gif\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.gifv\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.webp\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1089\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_loc(label)\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Post Text'"
     ]
    }
   ],
   "source": [
    "# Define a function to check if a post is an image-only post\n",
    "def is_image_only_post(post):\n",
    "    # Check if the post's content (Post Text) is empty (no text) and the URL is an image or an image-hosting site\n",
    "    return not post[\"Post Text\"] and post[\"Post URL\"].endswith(('.jpg', '.jpeg', '.png', '.gif', '.gifv', '.webp'))\n",
    "\n",
    "# Apply the function to the DataFrame to create a Boolean mask\n",
    "image_only_mask = df_reddit_post.apply(is_image_only_post, axis=1)\n",
    "\n",
    "# Filter out image-only posts\n",
    "filtered_df_reddit_post = df_reddit_post[~image_only_mask]\n",
    "print(f\"Total number of posts after filter collected: {len(filtered_df_reddit_post)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529e7013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the selected columns\n",
    "selected_columns = [\"Title\", \"Post Text\", \"Subreddit\"]\n",
    "filtered_columns_df_reddit_post = filtered_df_reddit_post[selected_columns]\n",
    "# Save the data to a CSV file\n",
    "filtered_columns_df_reddit_post.to_csv(\"reddit_posts (no image-only post).csv\", index=False)\n",
    "filtered_columns_df_reddit_post.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f9fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through all columns and convert text data to lowercase\n",
    "for column in filtered_columns_df_reddit_post.columns:\n",
    "    if filtered_columns_df_reddit_post[column].dtype == 'object':\n",
    "        filtered_columns_df_reddit_post[column] = filtered_columns_df_reddit_post[column].str.lower()\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "filtered_columns_df_reddit_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db5ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ac412b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Function to filter out stopwords\n",
    "def remove_stopwords(text):\n",
    "    if isinstance(text, str):  # Check if the value is a string (not NaN)\n",
    "        words = text.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        return \" \".join(filtered_words)\n",
    "    else:\n",
    "        return text  # Return the original value if it's NaN\n",
    "\n",
    "# Apply the function to the \"Title\" and \"Post Text\" columns using .loc\n",
    "filtered_columns_df_reddit_post[\"Stopword Dropped Title\"] = filtered_columns_df_reddit_post[\"Title\"].apply(remove_stopwords)\n",
    "filtered_columns_df_reddit_post[\"Stopword Dropped Post Text\"] = filtered_columns_df_reddit_post[\"Post Text\"].apply(remove_stopwords)\n",
    "\n",
    "# Create a clean copy of the DataFrame with the dropped columns\n",
    "dropped_filtered_columns_dataframe = filtered_columns_df_reddit_post.drop(columns=[\"Title\", \"Post Text\"]).copy()\n",
    "\n",
    "# Display the DataFrame with the dropped columns\n",
    "dropped_filtered_columns_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bb4374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Create a function to process and modify text\n",
    "def process_text(text):\n",
    "    if isinstance(text, str):\n",
    "        doc = nlp(text)\n",
    "        modified_tokens = []\n",
    "        for token in doc:\n",
    "            if token.text.lower() == \"i'm\":\n",
    "                modified_tokens.extend([\"i\", \"am\"])\n",
    "            elif token.text.lower() == \"emma's\":\n",
    "                modified_tokens.append(\"emma\")\n",
    "            else:\n",
    "                modified_tokens.append(token.text)\n",
    "        return \" \".join(modified_tokens)\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Loop through all columns and apply the processing function\n",
    "for column in dropped_filtered_columns_dataframe.columns:\n",
    "    if dropped_filtered_columns_dataframe[column].dtype == 'object':\n",
    "        dropped_filtered_columns_dataframe[column] = dropped_filtered_columns_dataframe[column].apply(process_text)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "dropped_filtered_columns_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4ddf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation from specific columns\n",
    "columns_to_clean = ['Stopword Dropped Post Text', 'Stopword Dropped Title']\n",
    "\n",
    "for column in columns_to_clean:\n",
    "    dropped_filtered_columns_dataframe[column] = dropped_filtered_columns_dataframe[column].str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "\n",
    "# Display the DataFrame\n",
    "dropped_filtered_columns_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c907fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is to split the content of each title and post into their unigrams and bigrams\n",
    "\n",
    "# Initialize the Porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Initialize the Porter stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to stem text\n",
    "def stem_text(text):\n",
    "    if isinstance(text, str):  # Check if text is a string\n",
    "        words = text.split()\n",
    "        stemmed_words = [stemmer.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    else:\n",
    "        return text  # Return the original value if it's not a string\n",
    "\n",
    "# Initialize the WordNet lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function to lemmatize text\n",
    "def lemmatize_text(text):\n",
    "    if isinstance(text, str):  # Check if text is a string\n",
    "        words = text.split()\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "        return \" \".join(lemmatized_words)\n",
    "    else:\n",
    "        return text  # Return the original value if it's not a string\n",
    "\n",
    "# Function to split text into unigrams and bigrams\n",
    "def split_text(text):\n",
    "    if isinstance(text, str):  # Check if text is a string\n",
    "        # Tokenize the text into words\n",
    "        words = text.split()\n",
    "        \n",
    "        # Create unigrams (single words)\n",
    "        unigrams = words\n",
    "        \n",
    "        # Create bigrams (consecutive word pairs)\n",
    "        bigrams = list(zip(words, words[1:]))\n",
    "        \n",
    "        return unigrams, bigrams\n",
    "    else:\n",
    "        return [], []  # Return empty lists for unigrams and bigrams if text is not a string\n",
    "\n",
    "# Apply stemming, lemmatization, and generate unigrams and bigrams to your columns\n",
    "dropped_filtered_columns_dataframe[\"Stemmed Title\"] = dropped_filtered_columns_dataframe[\"Stopword Dropped Title\"].apply(stem_text)\n",
    "dropped_filtered_columns_dataframe[\"Stemmed Post Text\"] = dropped_filtered_columns_dataframe[\"Stopword Dropped Post Text\"].apply(stem_text)\n",
    "dropped_filtered_columns_dataframe[\"Lemmatized Title\"] = dropped_filtered_columns_dataframe[\"Stopword Dropped Title\"].apply(lemmatize_text)\n",
    "dropped_filtered_columns_dataframe[\"Lemmatized Post Text\"] = dropped_filtered_columns_dataframe[\"Stopword Dropped Post Text\"].apply(lemmatize_text)\n",
    "dropped_filtered_columns_dataframe[[\"Unigrams Title\", \"Bigrams Title\"]] = dropped_filtered_columns_dataframe[\"Stopword Dropped Title\"].apply(split_text).apply(pd.Series)\n",
    "dropped_filtered_columns_dataframe[[\"Unigrams Post Text\", \"Bigrams Post Text\"]] = dropped_filtered_columns_dataframe[\"Stopword Dropped Post Text\"].apply(split_text).apply(pd.Series)\n",
    "\n",
    "# Drop the original columns\n",
    "dropped_filtered_columns_dataframe = dropped_filtered_columns_dataframe.drop(columns=[\"Stopword Dropped Title\", \"Stopword Dropped Post Text\"])\n",
    "\n",
    "# Convert all text columns to lowercase\n",
    "text_columns = [\"Stemmed Title\", \"Stemmed Post Text\", \"Lemmatized Title\", \"Lemmatized Post Text\"]\n",
    "for column in text_columns:\n",
    "    dropped_filtered_columns_dataframe[column] = dropped_filtered_columns_dataframe[column].str.lower()\n",
    "\n",
    "# Display the updated DataFrame\n",
    "dropped_filtered_columns_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cabf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the text from both \"Unigrams Title\" and \"Unigrams Post Text\" into a single column\n",
    "dropped_filtered_columns_dataframe['Combined Unigrams'] = dropped_filtered_columns_dataframe['Unigrams Title'] + dropped_filtered_columns_dataframe['Unigrams Post Text']\n",
    "\n",
    "# Convert the lists of unigrams into strings\n",
    "dropped_filtered_columns_dataframe['Combined Unigrams'] = dropped_filtered_columns_dataframe['Combined Unigrams'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the combined unigrams\n",
    "unigram_counts = vectorizer.fit_transform(dropped_filtered_columns_dataframe['Combined Unigrams'])\n",
    "\n",
    "# Get the feature names (unigrams)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame to store the counts\n",
    "unigram_counts_df = pd.DataFrame(unigram_counts.toarray(), columns=feature_names)\n",
    "\n",
    "# Add the subreddit column back to the DataFrame\n",
    "unigram_counts_df['Subreddit'] = dropped_filtered_columns_dataframe['Subreddit']\n",
    "\n",
    "# Group the DataFrame by \"Subreddit\" and sum the counts\n",
    "grouped_unigram_counts = unigram_counts_df.groupby('Subreddit').sum()\n",
    "\n",
    "# Get a list of unique subreddits\n",
    "unique_subreddits = dropped_filtered_columns_dataframe['Subreddit'].unique()\n",
    "\n",
    "# Dictionary to store unigram counts for each subreddit\n",
    "subreddit_unigram_counts = {}\n",
    "\n",
    "# Loop through each unique subreddit\n",
    "for subreddit_name in unique_subreddits:\n",
    "    # Get the specific unigram count for the subreddit\n",
    "    specific_unigram_count = grouped_unigram_counts.loc[subreddit_name]\n",
    "    \n",
    "    # To get the top N most common unigrams for a subreddit, you can use:\n",
    "    top_n = 10  # Replace with the desired number\n",
    "    top_n_unigrams = specific_unigram_count.nlargest(top_n)\n",
    "    \n",
    "    # Store the top N unigrams in the dictionary\n",
    "    subreddit_unigram_counts[subreddit_name] = top_n_unigrams\n",
    "\n",
    "# Loop through each subreddit and its top N unigrams\n",
    "for subreddit_name, top_n_unigrams in subreddit_unigram_counts.items():\n",
    "    print(f\"Subreddit: {subreddit_name}\")\n",
    "    print(top_n_unigrams)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746c7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a DataFrame\n",
    "df_unigrams = pd.DataFrame(subreddit_unigram_counts)\n",
    "\n",
    "# Plot the data as a bar chart\n",
    "ax = df_unigrams.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Top Unigrams in Subreddits')\n",
    "plt.xlabel('Unigrams')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0fa2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CountVectorizer for bigrams\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "bigram_counts = bigram_vectorizer.fit_transform(dropped_filtered_columns_dataframe['Combined Unigrams'])\n",
    "bigram_feature_names = bigram_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame to store the bigram counts\n",
    "bigram_counts_df = pd.DataFrame(bigram_counts.toarray(), columns=bigram_feature_names)\n",
    "\n",
    "# Add the 'Subreddit' column back to the DataFrame\n",
    "bigram_counts_df['Subreddit'] = dropped_filtered_columns_dataframe['Subreddit']\n",
    "\n",
    "# Group the DataFrame by 'Subreddit' and sum the counts\n",
    "grouped_bigram_counts = bigram_counts_df.groupby('Subreddit').sum()\n",
    "\n",
    "# Get a list of unique subreddits\n",
    "unique_subreddits = dropped_filtered_columns_dataframe['Subreddit'].unique()\n",
    "\n",
    "# Dictionary to store bigram counts for each subreddit\n",
    "subreddit_bigram_counts = {}\n",
    "\n",
    "# Loop through each unique subreddit\n",
    "for subreddit_name in unique_subreddits:\n",
    "    # Get the specific bigram count for the subreddit\n",
    "    specific_bigram_count = grouped_bigram_counts.loc[subreddit_name]\n",
    "    \n",
    "    # To get the top N most common bigrams for a subreddit, you can use:\n",
    "    top_n = 10  # Replace with the desired number\n",
    "    top_n_bigrams = specific_bigram_count.nlargest(top_n)\n",
    "    \n",
    "    # Store the top N bigrams in the dictionary\n",
    "    subreddit_bigram_counts[subreddit_name] = top_n_bigrams\n",
    "\n",
    "# Loop through each subreddit and its top N bigrams\n",
    "for subreddit_name, top_n_bigrams in subreddit_bigram_counts.items():\n",
    "    print(f\"Subreddit: {subreddit_name}\")\n",
    "    print(top_n_bigrams)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bdad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a DataFrame\n",
    "df_bigram = pd.DataFrame(subreddit_bigram_counts)\n",
    "\n",
    "# Plot the data as a bar chart\n",
    "ax = df_bigram.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Top Bigrams in Subreddits')\n",
    "plt.xlabel('Bigrams')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a2bd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CountVectorizer for trigrams\n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "trigram_counts = trigram_vectorizer.fit_transform(dropped_filtered_columns_dataframe['Combined Unigrams'])\n",
    "trigram_feature_names = trigram_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame to store the trigram counts\n",
    "trigram_counts_df = pd.DataFrame(trigram_counts.toarray(), columns=trigram_feature_names)\n",
    "\n",
    "# Add the 'Subreddit' column back to the DataFrame\n",
    "trigram_counts_df['Subreddit'] = dropped_filtered_columns_dataframe['Subreddit']\n",
    "\n",
    "# Group the DataFrame by 'Subreddit' and sum the counts\n",
    "grouped_trigram_counts = trigram_counts_df.groupby('Subreddit').sum()\n",
    "\n",
    "# Get a list of unique subreddits\n",
    "unique_subreddits = dropped_filtered_columns_dataframe['Subreddit'].unique()\n",
    "\n",
    "# Dictionary to store trigram counts for each subreddit\n",
    "subreddit_trigram_counts = {}\n",
    "\n",
    "# Loop through each unique subreddit\n",
    "for subreddit_name in unique_subreddits:\n",
    "    # Get the specific trigram count for the subreddit\n",
    "    specific_trigram_count = grouped_trigram_counts.loc[subreddit_name]\n",
    "    \n",
    "    # To get the top N most common trigrams for a subreddit, you can use:\n",
    "    top_n = 10  # Replace with the desired number\n",
    "    top_n_trigrams = specific_trigram_count.nlargest(top_n)\n",
    "    \n",
    "    # Store the top N trigrams in the dictionary\n",
    "    subreddit_trigram_counts[subreddit_name] = top_n_trigrams\n",
    "\n",
    "# Loop through each subreddit and its top N trigrams\n",
    "for subreddit_name, top_n_trigrams in subreddit_trigram_counts.items():\n",
    "    print(f\"Subreddit: {subreddit_name}\")\n",
    "    print(top_n_trigrams)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c5e57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary to a DataFrame\n",
    "df_trigram = pd.DataFrame(subreddit_trigram_counts)\n",
    "\n",
    "# Plot the data as a bar chart\n",
    "ax = df_trigram.plot(kind='bar', figsize=(10, 6))\n",
    "plt.title('Top Trigrams in Subreddits')\n",
    "plt.xlabel('Trigrams')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(ha='right')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
